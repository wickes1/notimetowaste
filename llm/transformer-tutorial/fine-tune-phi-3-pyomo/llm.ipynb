{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Phi-3 for Pyomo Code Generation\n",
    "\n",
    "This notebook documents the process of fine-tuning the `microsoft/Phi-3-mini-128k-instruct` model with Quantized LoRA (QLoRA) to improve its capability in generating Pyomo code based on user inputs and existing code contexts. The dataset was generated using GPT-4o and includes 72 training examples, 8 evaluation examples, and 20 test examples.\n",
    "\n",
    "The training process utilizes QLoRA for efficient memory usage and the final model is saved and evaluated based on metrics such as faithfulness, answer relevancy, and code quality.\n",
    "\n",
    "The project structure includes:\n",
    "- `datasets/test`: Test dataset\n",
    "- `datasets/train`: Training and evaluation datasets\n",
    "- `checkpoints`: Fine-tuned model checkpoints\n",
    "- `merged_model`: PEFT model merged with the original model\n",
    "\n",
    "Evaluation shows significant improvement in faithfulness and answer similarity metrics after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import nest_asyncio\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ROCm for GFX_1030\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"10.3.0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "api = HfApi()\n",
    "hf_username = api.whoami()[\"name\"]\n",
    "\n",
    "# Debugging with unknown package error\n",
    "# import logging\n",
    "\n",
    "# # Set the logging level to DEBUG\n",
    "# logging.basicConfig(\n",
    "#     level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amdgpu.ids: No such file or directory\n",
      "amdgpu.ids: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version 2.3.1+rocm6.0\n",
      "bitsandbytes version 0.43.2.dev\n",
      "working on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")\n",
    "print(f\"bitsandbytes version {bnb.__version__}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"wickes1/Phi-3-mini-128k-instruct-pyomo\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyomo.environ import *\n",
      "\n",
      "model = ConcreteModel()\n",
      "model.Q = Var(domain=NonNegativeReals)\n",
      "model.obj = Objective(expr=2 * model.Q, sense=maximize)\n",
      "\n",
      "# Define constraints\n",
      "model.constraint1 = Constraint(expr=model.Q <= 8)\n",
      "model.constraint2 = Constraint(expr=model.Q >= 10)\n",
      "\n",
      "# Solve the model\n",
      "solver = SolverFactory('glpk')\n",
      "result = solver.solve(model)\n",
      "<|end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "input_text = \"Add a constraint to ensure variable Q is at least 10.\"\n",
    "context_text = \"from pyomo.environ import *\\n\\nmodel = ConcreteModel()\\nmodel.Q = Var(domain=NonNegativeReals)\\nmodel.obj = Objective(expr=2 * model.Q, sense=maximize)\\n\\n# Define constraints\\nmodel.constraint1 = Constraint(expr=model.Q <= 8)\\n\\n# Solve the model\\nsolver = SolverFactory('glpk')\\nresult = solver.solve(model)\"\n",
    "prompt = \"<|system|>\\nYou are a highly knowledgeable AI assistant specializing in Python programming and Pyomo models. Based on the user input question and the provided code snippet, your task is to generate the complete Python code needed to address the user's query.\\n\\nInstructions:\\n1. Review the user input question carefully.\\n2. Analyze the provided code snippet within the context given.\\n3. Generate the full Python code required to answer the user's question, ensuring the code is functional and correctly formatted.\\n\\nNotes:\\n- Ensure that the generated code is properly indented and follows Python best practices.\\n- Include necessary import statements and any relevant comments to enhance code readability.\\n- Verify that the code aligns with the user's specific requirements as described in the input and context.\\n<|end|>\\n<|user|>\\nInput:\\n{input}\\n\\nContext:\\n{context}\\n\\nComplete Python Code:\\n<|end|>\\n<|assistant|>\"\n",
    "\n",
    "# Tokenize input\n",
    "model_input = tokenizer(\n",
    "    prompt.format(input=input_text, context=context_text), return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    model_input.input_ids,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Prompt generation functions\n",
    "\n",
    "\n",
    "def generate_prompt_str(data_point):\n",
    "    prompt = f\"\"\"\n",
    "<|system|>\n",
    "You are a highly knowledgeable AI assistant specializing in Python programming and Pyomo models. Based on the user input question and the provided code snippet, your task is to generate the complete Python code needed to address the user's query.\n",
    "\n",
    "Instructions:\n",
    "1. Review the user input question carefully.\n",
    "2. Analyze the provided code snippet within the context given.\n",
    "3. Generate the full Python code required to answer the user's question, ensuring the code is functional and correctly formatted.\n",
    "\n",
    "Notes:\n",
    "- Ensure that the generated code is properly indented and follows Python best practices.\n",
    "- Include necessary import statements and any relevant comments to enhance code readability.\n",
    "- Verify that the code aligns with the user's specific requirements as described in the input and context.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "Complete Python Code:\n",
    "<|end|>\n",
    "<|assistant|>{data_point[\"output\"]}<|endoftext|>\n",
    "\"\"\".strip()\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "def generate_test_prompt_str(data_point):\n",
    "    prompt = f\"\"\"\n",
    "<|system|>\n",
    "You are a highly knowledgeable AI assistant specializing in Python programming and Pyomo models. Based on the user input question and the provided code snippet, your task is to generate the complete Python code needed to address the user's query.\n",
    "\n",
    "Instructions:\n",
    "1. Review the user input question carefully.\n",
    "2. Analyze the provided code snippet within the context given.\n",
    "3. Generate the full Python code required to answer the user's question, ensuring the code is functional and correctly formatted.\n",
    "\n",
    "Notes:\n",
    "- Ensure that the generated code is properly indented and follows Python best practices.\n",
    "- Include necessary import statements and any relevant comments to enhance code readability.\n",
    "- Verify that the code aligns with the user's specific requirements as described in the input and context.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "Complete Python Code:\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\".strip()\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load dataset\n",
    "# data_files = {\"train\": \"./datasets/train/*.csv\", \"test\": \"./datasets/test/*.csv\"}\n",
    "# dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "# train_test_split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# train_dataset = train_test_split[\"train\"]\n",
    "# eval_dataset = train_test_split[\"test\"]\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "# # Apply the prompt generation functions using map\n",
    "# train_dataset = train_dataset.map(generate_prompt_str)\n",
    "# test_dataset = test_dataset.map(generate_test_prompt_str)\n",
    "# eval_dataset = eval_dataset.map(generate_prompt_str)\n",
    "\n",
    "# # Verify\n",
    "# print(train_dataset)\n",
    "# print(eval_dataset)\n",
    "# print(test_dataset)\n",
    "\n",
    "# dataset.push_to_hub(f\"{hf_user}/pyomo-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'context', 'output', 'text'],\n",
      "    num_rows: 72\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input', 'context', 'output', 'text'],\n",
      "    num_rows: 8\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input', 'context', 'output', 'text'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remote Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"{hf_username}/pyomo-100\")\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"].map(generate_prompt_str)\n",
    "eval_dataset = train_test_split[\"test\"].map(generate_prompt_str)\n",
    "test_dataset = dataset[\"test\"].map(generate_test_prompt_str)\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='wickes1/Phi-3-mini-128k-instruct-pyomo', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"  # https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# checkpoint_dir = \"./checkpoints/\" # First time training\n",
    "# checkpoint_dir = \"./merged_model/\"  # Continue from merged model\n",
    "checkpoint_dir = f\"{hf_username}/Phi-3-mini-128k-instruct-pyomo\"  # Fine-tuned model\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac440d5f30a542d088c85c9c399b0a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d076c5802f784f9dbb63c39a4e23f293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    bias=\"none\",\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    bf16=False,\n",
    "    evaluation_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    group_by_length=True,\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    logging_steps=25,  # log every 10 steps\n",
    "    lr_scheduler_type=\"cosine\",  # use cosine learning rate scheduler\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=3,  # number of training epochs\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    output_dir=checkpoint_dir,  # directory to save and repository id\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    resume_from_checkpoint=True,\n",
    "    save_steps=10,\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": True,\n",
    "        # \"append_concat_token\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = AMD Radeon Graphics. Max memory = 15.984 GB.\n",
      "5.072 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# trainer_stats = trainer.train()\n",
    "\n",
    "# trainer.save_model(\"./checkpoints\")\n",
    "# tokenizer.save_pretrained(\"./checkpoints\")\n",
    "# trainer.save_metrics(\"metrics.json\")\n",
    "\n",
    "# tokenizer.push_to_hub(f\"{hf_username}/Phi-3-mini-128k-instruct-pyomo\", private=True)\n",
    "# trainer.push_to_hub(f\"{hf_username}/Phi-3-mini-128k-instruct-pyomo\", private=True)\n",
    "# trainer_stats.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fce730aef84a9cbb031b0354eade04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Merge PEFT model with the original model\n",
    "# import gc\n",
    "\n",
    "# import torch\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# for _ in range(100):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# compute_dtype = getattr(torch, \"float16\")\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "# )\n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     \"./checkpoints/\",  # PEFT Fine-tuned model\n",
    "#     torch_dtype=compute_dtype,\n",
    "#     return_dict=False,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     device_map=device,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(\n",
    "#     \"./merged_model\", safe_serialization=True, max_shard_size=\"2GB\"\n",
    "# )\n",
    "# tokenizer.save_pretrained(\"./merged_model\")\n",
    "\n",
    "# merged_model.push_to_hub(f\"{hf_username}/Phi-3-mini-128k-instruct-pyomo\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_answer at 0x76147fbde660> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131e5a2e3bea48e9980d14eae8b8d62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(sample):\n",
    "    test_prompt = generate_test_prompt_str(sample)[\"text\"]\n",
    "    model_input = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    input_ids = model_input.input_ids.to(model.device)\n",
    "    attention_mask = model_input.attention_mask.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_token = model.generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=512\n",
    "        )\n",
    "    prompt_length = model_input.input_ids.shape[-1]\n",
    "    gen_token_without_prompt = gen_token[:, prompt_length:]\n",
    "    gen_text = tokenizer.decode(gen_token_without_prompt[0], skip_special_tokens=True)\n",
    "    sample[\"answer\"] = gen_text\n",
    "    return sample\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.map(generate_answer, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyomo.environ import *\n",
      "\n",
      "model = ConcreteModel()\n",
      "model.x = Var(domain=NonNegativeReals)\n",
      "model.y = Var(domain=NonNegativeReals)\n",
      "model.obj = Objective(expr=2 * model.x + 3 * model.y, sense=maximize)\n",
      "\n",
      "# Define constraints\n",
      "model.constraint1 = Constraint(expr=model.x + 2 * model.y <= 8)\n",
      "model.constraint2 = Constraint(expr=3 * model.x + 2 * model.y <= 12)\n",
      "model.constraint3 = Constraint(expr=3 * model.x + 6 * model.y <= 50)\n",
      "\n",
      "# Solve the model\n",
      "solver = SolverFactory('glpk')\n",
      "result = solver.solve(model)\n",
      "<|end|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_sample = test_dataset[random.randint(0, len(test_dataset) - 1)]\n",
    "gen_token = test_sample[\"output\"]\n",
    "\n",
    "test_prompt = generate_test_prompt_str(test_sample)[\"text\"]\n",
    "model_input = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "gen_token = model.generate(\n",
    "    model_input.input_ids,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "prompt_length = model_input.input_ids.shape[-1]\n",
    "gen_token_without_prompt = gen_token[:, prompt_length:]\n",
    "gen_text = tokenizer.decode(gen_token_without_prompt[0], skip_special_tokens=True)\n",
    "# print(gen_text)\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# # execute the code\n",
    "# result = subprocess.run([sys.executable, \"-c\", gen_text], capture_output=True)\n",
    "# print(result.stdout.decode(\"utf-8\"))\n",
    "# print(result.stderr.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to generate text from the model\n",
    "\n",
    "# M1\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# generation_args = {\n",
    "#     \"max_new_tokens\": 250,\n",
    "#     \"return_full_text\": False,\n",
    "#     \"temperature\": 0,\n",
    "#     \"do_sample\": False,\n",
    "# }\n",
    "\n",
    "# for out in pipe(test_prompt, **generation_args):\n",
    "#     print(out[\"generated_text\"], end=\"\", flush=True)\n",
    "\n",
    "# M2\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1000)[0], skip_special_tokens=True))\n",
    "\n",
    "# M3\n",
    "# outputs = model.generate(input_ids=model_input.input_ids, max_new_tokens=250)\n",
    "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "\n",
    "# outputs = model.generate(input_ids=model_input.input_ids, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95)\n",
    "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "\n",
    "# M4\n",
    "# stop_token = \"<|endoftext|>\"\n",
    "# stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "# model_input = tokenizer(\"hi\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# _ = model.generate(\n",
    "#     model_input.input_ids,\n",
    "#     streamer=text_streamer,\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=stop_token_id,\n",
    "#     do_sample=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Ragas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b9518b2b364111b677b0a01361c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load PreTrained Model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "pretrained_model = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "finetuned_model = f\"{hf_username}/Phi-3-mini-128k-instruct-pyomo\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "dataset = copy.deepcopy(test_dataset)\n",
    "dataset = dataset.rename_columns(\n",
    "    {\"input\": \"question\", \"output\": \"ground_truth\", \"context\": \"contexts\"}\n",
    ")\n",
    "dataset\n",
    "\n",
    "# convert contexts to list\n",
    "dataset = dataset.map(lambda x: {\"contexts\": [x[\"contexts\"]]})\n",
    "# dataset.to_pandas() # Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646832808d934eff9a55e0737400a9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8494, 'answer_relevancy': 0.5697, 'answer_similarity': 0.9994, 'correctness': 0.9000, 'code-quality': 0.9000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import ragas.metrics\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    AspectCritique,\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "from ragas.metrics.critique import correctness, harmfulness\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "importlib.reload(ragas.metrics)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "AZURE_API_VERSION = \"2024-02-01\"\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# AZURE_MODEL= 'gpt-35-turbo-16k'\n",
    "AZURE_MODEL = \"gpt-4-32k\"\n",
    "azure_model = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_MODEL,\n",
    "    model=AZURE_MODEL,\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    # model=\"text-embedding-3-small\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    tiktoken_enabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "code_quality = AspectCritique(\n",
    "    name=\"code-quality\",\n",
    "    definition=\"Is the submission a valid code that well-structured, efficient, and free from errors?\",\n",
    "    strictness=1,  # Number of self check\n",
    ")\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    # LLM metrics\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    correctness,\n",
    "    code_quality,\n",
    "    # RAG metrics\n",
    "    # context_recall,\n",
    "    # context_precision,\n",
    "    # harmfulness,\n",
    "]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=metrics,\n",
    "    llm=azure_model,\n",
    "    embeddings=ollama_embeddings,\n",
    "    is_async=True,\n",
    "    run_config=RunConfig(timeout=30, max_wait=30, max_retries=2, max_workers=4),\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export mean value to json\n",
    "FILE_NAME = \"evaluation-finetuned\"\n",
    "# FILE_NAME = \"evaluation-pretrained\"\n",
    "result.scores.to_pandas().mean().to_json(f\"{FILE_NAME}.json\", orient=\"index\", indent=4)\n",
    "result.to_pandas().to_csv(f\"{FILE_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>code-quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>add a constraint to ensure Q is at least 10.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = ConcreteModel()\\nmodel.Q = Var(domain=NonNegativeReals)\\nmodel.obj = Objective(expr=2 * model.Q, sense=maximize)\\n\\n# Define constraints\\nmodel.constraint1 = Constraint(expr=model.Q &lt;= 8)\\n\\n# Solve the model\\nsolver = SolverFactory('glpk')\\nresult = solver.solve(model)]</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = ConcreteModel()\\nmodel.Q = Var(domain=NonNegativeReals)\\nmodel.obj = Objective(expr=2 * model.Q, sense=maximize)\\n\\n# Define constraints\\nmodel.constraint1 = Constraint(expr=model.Q &lt;= 8)\\nmodel.constraint2 = Constraint(expr=model.Q &gt;= 10)\\n\\n# Solve the model\\nsolver = SolverFactory('glpk')\\nresult = solver.solve(model)</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI assistant specializing in Python programming and Pyomo models. Based on the user input question and the provided code snippet, your task is to generate the complete Python code needed to address the user's query.\\n\\nInstructions:\\n1. Review the user input question carefully.\\n2. Analyze the provided code snippet within the context given.\\n3. Generate the full Python code required to answer the user's question, ensuring the code is functional and correctly formatted.\\n\\nNotes:\\n- Ensure that the generated code is properly indented and follows Python best practices.\\n- Include necessary import statements and any relevant comments to enhance code readability.\\n- Verify that the code aligns with the user's specific requirements as described in the input and context.\\n&lt;|end|&gt;\\n&lt;|user|&gt;\\nInput:\\nadd a constraint to ensure Q is at least 10.\\n\\nContext:\\nfrom pyomo.environ import *\\n\\nmodel = ConcreteModel()\\nmodel.Q = Var(domain=NonNegativeReals)\\nmodel.obj = Objective(expr=2 * model.Q, sense=maximize)\\n\\n# Define constraints\\nmodel.constraint1 = Constraint(expr=model.Q &lt;= 8)\\n\\n# Solve the model\\nsolver = SolverFactory('glpk')\\nresult = solver.solve(model)\\n\\nComplete Python Code:\\n&lt;|end|&gt;\\n&lt;|assistant|&gt;</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = ConcreteModel()\\nmodel.Q = Var(domain=NonNegativeReals)\\nmodel.obj = Objective(expr=2 * model.Q, sense=maximize)\\n\\n# Define constraints\\nmodel.constraint1 = Constraint(expr=model.Q &lt;= 8)\\nmodel.constraint2 = Constraint(expr=model.Q &gt;= 10)\\n\\n# Solve the model\\nsolver = SolverFactory('glpk')\\nresult = solver.solve(model)\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "df = result.to_pandas()\n",
    "sample = df[df[\"code-quality\"] == 0]\n",
    "\n",
    "display(HTML(sample.to_html(max_rows=10, max_cols=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>correctness</th>\n",
       "      <th>code-quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add a new variable P and a constraint P + Y &gt;= 8.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.530712</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modify the objective function to maximize 3X +...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.451872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change the domain of Z to NonNegativeIntegers.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.479246</td>\n",
       "      <td>0.998503</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>add a parameter G with value 25 and use it in ...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.551395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>introduce a new constraint 5X + Y + Z &lt;= 60.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.475206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>change the solver from 'scip' to 'couenne'.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.683909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>add a constraint to ensure P is at most 20.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.559796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>remove the constraint X + Y - Z &gt;= 5.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.453050</td>\n",
       "      <td>0.998652</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>change the sense of the objective function to ...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.604279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>add a parameter H with value 14 and use it in ...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.495061</td>\n",
       "      <td>0.997958</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>add a new variable Q and a constraint Q - X &gt;= 2.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.516418</td>\n",
       "      <td>0.999601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>modify the objective function to minimize 4X +...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.549022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>change the domain of T to NonNegativeReals.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.461204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>add a parameter I with value 6 and use it in a...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.431182</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>introduce a new constraint X + 2Y + Z &lt;= 40.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.477029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>change the solver from 'couenne' to 'gurobi'.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.674589</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>add a constraint to ensure Q is at least 10.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.572492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>remove the constraint 5X + 4Y &lt;= 28.</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.531436</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>change the sense of the objective function to ...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.618948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>add a parameter J with value 8 and use it in a...</td>\n",
       "      <td>[from pyomo.environ import *\\n\\nmodel = Concre...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>&lt;|system|&gt;\\nYou are a highly knowledgeable AI ...</td>\n",
       "      <td>from pyomo.environ import *\\n\\nmodel = Concret...</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.588727</td>\n",
       "      <td>0.998455</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   add a new variable P and a constraint P + Y >= 8.   \n",
       "1   modify the objective function to maximize 3X +...   \n",
       "2      change the domain of Z to NonNegativeIntegers.   \n",
       "3   add a parameter G with value 25 and use it in ...   \n",
       "4        introduce a new constraint 5X + Y + Z <= 60.   \n",
       "5         change the solver from 'scip' to 'couenne'.   \n",
       "6         add a constraint to ensure P is at most 20.   \n",
       "7               remove the constraint X + Y - Z >= 5.   \n",
       "8   change the sense of the objective function to ...   \n",
       "9   add a parameter H with value 14 and use it in ...   \n",
       "10  add a new variable Q and a constraint Q - X >= 2.   \n",
       "11  modify the objective function to minimize 4X +...   \n",
       "12        change the domain of T to NonNegativeReals.   \n",
       "13  add a parameter I with value 6 and use it in a...   \n",
       "14       introduce a new constraint X + 2Y + Z <= 40.   \n",
       "15      change the solver from 'couenne' to 'gurobi'.   \n",
       "16       add a constraint to ensure Q is at least 10.   \n",
       "17               remove the constraint 5X + 4Y <= 28.   \n",
       "18  change the sense of the objective function to ...   \n",
       "19  add a parameter J with value 8 and use it in a...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "1   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "2   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "3   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "4   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "5   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "6   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "7   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "8   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "9   [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "10  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "11  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "12  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "13  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "14  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "15  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "16  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "17  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "18  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "19  [from pyomo.environ import *\\n\\nmodel = Concre...   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "1   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "2   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "3   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "4   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "5   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "6   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "7   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "8   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "9   from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "10  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "11  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "12  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "13  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "14  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "15  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "16  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "17  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "18  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "19  from pyomo.environ import *\\n\\nmodel = Concret...   \n",
       "\n",
       "                                                 text  \\\n",
       "0   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "1   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "2   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "3   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "4   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "5   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "6   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "7   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "8   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "9   <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "10  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "11  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "12  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "13  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "14  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "15  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "16  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "17  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "18  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "19  <|system|>\\nYou are a highly knowledgeable AI ...   \n",
       "\n",
       "                                               answer  faithfulness  \\\n",
       "0   from pyomo.environ import *\\n\\nmodel = Concret...      0.750000   \n",
       "1   from pyomo.environ import *\\n\\nmodel = Concret...      0.777778   \n",
       "2   from pyomo.environ import *\\n\\nmodel = Concret...      0.600000   \n",
       "3   from pyomo.environ import *\\n\\nmodel = Concret...      0.818182   \n",
       "4   from pyomo.environ import *\\n\\nmodel = Concret...      0.700000   \n",
       "5   from pyomo.environ import *\\n\\nmodel = Concret...      0.500000   \n",
       "6   from pyomo.environ import *\\n\\nmodel = Concret...      0.666667   \n",
       "7   from pyomo.environ import *\\n\\nmodel = Concret...      0.875000   \n",
       "8   from pyomo.environ import *\\n\\nmodel = Concret...      0.833333   \n",
       "9   from pyomo.environ import *\\n\\nmodel = Concret...      0.727273   \n",
       "10  from pyomo.environ import *\\n\\nmodel = Concret...      0.733333   \n",
       "11  from pyomo.environ import *\\n\\nmodel = Concret...      0.777778   \n",
       "12  from pyomo.environ import *\\n\\nmodel = Concret...      0.888889   \n",
       "13  from pyomo.environ import *\\n\\nmodel = Concret...      0.727273   \n",
       "14  from pyomo.environ import *\\n\\nmodel = Concret...      0.727273   \n",
       "15  from pyomo.environ import *\\n\\nmodel = Concret...      0.818182   \n",
       "16  from pyomo.environ import *\\n\\nmodel = Concret...      0.333333   \n",
       "17  from pyomo.environ import *\\n\\nmodel = Concret...      0.888889   \n",
       "18  from pyomo.environ import *\\n\\nmodel = Concret...      0.777778   \n",
       "19  from pyomo.environ import *\\n\\nmodel = Concret...      0.818182   \n",
       "\n",
       "    answer_relevancy  answer_similarity  correctness  code-quality  \n",
       "0           0.530712           0.999089            1             1  \n",
       "1           0.451872           1.000000            1             1  \n",
       "2           0.479246           0.998503            1             1  \n",
       "3           0.551395           1.000000            1             1  \n",
       "4           0.475206           1.000000            1             1  \n",
       "5           0.683909           1.000000            1             1  \n",
       "6           0.559796           1.000000            1             1  \n",
       "7           0.453050           0.998652            1             1  \n",
       "8           0.604279           1.000000            1             1  \n",
       "9           0.495061           0.997958            1             1  \n",
       "10          0.516418           0.999601            1             1  \n",
       "11          0.549022           1.000000            1             1  \n",
       "12          0.461204           1.000000            1             1  \n",
       "13          0.431182           0.998226            1             1  \n",
       "14          0.477029           1.000000            1             1  \n",
       "15          0.674589           1.000000            1             1  \n",
       "16          0.572492           1.000000            0             0  \n",
       "17          0.531436           0.998056            1             1  \n",
       "18          0.618948           1.000000            1             1  \n",
       "19          0.588727           0.998455            1             1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
