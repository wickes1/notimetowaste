{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Agent Reasoning Loop\n",
    "\n",
    "- Function calling means the LLM is outputting structured format for the agent to call functions.\n",
    "- Suggesting you only use `GPT-3.5-Turbo` or `GPT-4` for function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# !wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from llama_index.core.indices import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "\n",
    "\n",
    "def get_doc_tools(\n",
    "    file_path: str,\n",
    "    name: str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "    def vector_query(query: str, page_numbers: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Use to answer questions over a given paper.\n",
    "\n",
    "        Useful if you have specific questions over the paper.\n",
    "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "\n",
    "        Args:\n",
    "            query (str): the string query to be embedded.\n",
    "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
    "                if we want to perform a vector search\n",
    "                over all pages. Otherwise, filter by the set of specified pages.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [{\"key\": \"page_label\", \"value\": p} for p in page_numbers]\n",
    "\n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts, condition=FilterCondition.OR\n",
    "            ),\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "\n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query\n",
    "    )\n",
    "\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"summary_tool_{name}\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(f\"Useful for summarization questions related to {name}\"),\n",
    "    )\n",
    "\n",
    "    return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core import Settings, SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "AZURE_API_ENDPOINT = os.environ.get(\"AZURE_API_ENDPOINT\")\n",
    "AZURE_API_KEY = os.environ.get(\"AZURE_API_KEY\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4-32k\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=AZURE_API_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    deployment_name=\"gpt-4-32k\",\n",
    "    temperature=0,\n",
    ")\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint=AZURE_API_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    api_version=\"2024-02-01\",\n",
    "    deployment_name=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(\"metagpt.pdf\", \"metagpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner, FunctionCallingAgentWorker\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "In MetaGPT, various roles are assigned to different agents, such as Product Manager, Architect, and Engineer. Each agent has a specific role and expertise, and they follow established standards. This allows for automatic requirement analysis, system design, code generation, modification, execution, and debugging during runtime. The addition of different roles, apart from just the Engineer, consistently improves both revisions and executability of the code. While more roles slightly increase the expenses, the overall performance improves noticeably.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"communication between agents in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "In the MetaGPT framework, agents can collaborate and communicate with each other. They share and comply with development and communication protocols defined within the framework. This allows for the execution of more complex tasks or projects by collecting several agents together.\n",
      "=== LLM Response ===\n",
      "In MetaGPT, different agents are assigned various roles such as Product Manager, Architect, and Engineer. Each agent has a specific role and expertise, and they follow established standards. This allows for automatic requirement analysis, system design, code generation, modification, execution, and debugging during runtime. The addition of different roles, apart from just the Engineer, consistently improves both revisions and executability of the code. While more roles slightly increase the expenses, the overall performance improves noticeably.\n",
      "\n",
      "As for communication, agents in the MetaGPT framework can collaborate and communicate with each other. They share and comply with development and communication protocols defined within the framework. This allows for the execution of more complex tasks or projects by collecting several agents together.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: metagpt.pdf\n",
      "file_path: metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-06-22\n",
      "last_modified_date: 2024-06-22\n",
      "\n",
      "Preprint\n",
      "Figure 1: The software development SOPs between MetaGPT and real-world human teams.\n",
      "In software engineering, SOPs promote collaboration among various roles. MetaGPT showcases\n",
      "its ability to decompose complex tasks into specific actionable procedures assigned to various roles\n",
      "(e.g., Product Manager, Architect, Engineer, etc.).\n",
      "documents, design artifacts, flowcharts, and interface specifications. The use of intermediate struc-\n",
      "tured outputs significantly increases the success rate of target code generation. Because it helps\n",
      "maintain consistency in communication, minimizing ambiguities and errors during collaboration.\n",
      "More graphically, in a company simulated by MetaGPT, all employees follow a strict and stream-\n",
      "lined workflow, and all their handovers must comply with certain established standards. This reduces\n",
      "the risk of hallucinations caused by idle chatter between LLMs, particularly in role-playing frame-\n",
      "works, like: “ Hi, hello and how are you?” – Alice (Product Manager); “ Great! Have you had\n",
      "lunch?” – Bob (Architect).\n",
      "Benefiting from SOPs, MetaGPT offers a promising approach to meta-programming. In this context,\n",
      "we adopt meta-programming1as ”programming to program”, in contrast to the broader fields of meta\n",
      "learning and ”learning to learn” (Schmidhuber, 1987; 1993a; Hochreiter et al., 2001; Schmidhuber,\n",
      "2006; Finn et al., 2017).\n",
      "This notion of meta-programming also encompasses earlier efforts like CodeBERT (Feng et al.,\n",
      "2020) and recent projects such as CodeLlama (Rozi `ere et al., 2023) and WizardCoder (Luo\n",
      "et al., 2023). However, MetaGPT stands out as a unique solution that allows for efficient meta-\n",
      "programming through a well-organized group of specialized agents. Each agent has a specific role\n",
      "and expertise, following some established standards. This allows for automatic requirement analysis,\n",
      "system design, code generation, modification, execution, and debugging during runtime, highlight-\n",
      "ing how agent-based techniques can enhance meta-programming.\n",
      "To validate the design of MetaGPT, we use publicly available HumanEval (Chen et al., 2021a) and\n",
      "MBPP (Austin et al., 2021) for evaluations. Notably, in code generation benchmarks, MetaGPT\n",
      "achieves a new state-of-the-art (SoTA) with 85.9% and 87.7% in Pass@1. When compared to other\n",
      "popular frameworks for creating complex software projects, such as AutoGPT (Torantulino et al.,\n",
      "2023), LangChain (Chase, 2022), AgentVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\n",
      "MetaGPT also stands out in handling higher levels of software complexity and offering extensive\n",
      "functionality. Remarkably, in our experimental evaluations, MetaGPT achieves a 100% task com-\n",
      "pletion rate, demonstrating the robustness and efficiency (time and token costs) of our design.\n",
      "We summarize our contributions as follows:\n",
      "1https://en.wikipedia.org/w/index.php?title=Metaprogramming\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"evaluation datasets used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in MetaGPT include the SoftwareDev dataset, HumanEval, and MBPP benchmarks.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT include:\n",
      "\n",
      "1. The SoftwareDev dataset: This dataset is used to evaluate the performance of the model in software development tasks.\n",
      "\n",
      "2. HumanEval: This is another dataset used for evaluation. It consists of a variety of programming tasks and is used to assess the model's ability to solve these tasks.\n",
      "\n",
      "3. MBPP benchmarks: These benchmarks are also used in the evaluation process to measure the model's performance.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me about the evaluation datasets used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the results over one of the above datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"results of MetaGPT on the SoftwareDev dataset\"}\n",
      "=== Function Output ===\n",
      "MetaGPT has shown impressive results on the SoftwareDev dataset. It achieved an average score of 3.9, surpassing ChatDev's score of 2.1. In terms of executability, MetaGPT scored 3.75, which is very close to 4 (flawless). It also took less time to run, with a time of 503 seconds, which is significantly less than ChatDev. Although MetaGPT required more tokens (24,613 or 31,255 compared to 19,292), it only needed 126.5/124.3 tokens to generate one line of code, while ChatDev used 248.9 tokens. The cost of human revision for MetaGPT was also lower. Furthermore, MetaGPT demonstrated superior performance when used with GPT-4 as the backend on selected SoftwareDev tasks.\n",
      "=== LLM Response ===\n",
      "MetaGPT has shown impressive results on the SoftwareDev dataset. It achieved an average score of 3.9, surpassing ChatDev's score of 2.1. In terms of executability, MetaGPT scored 3.75, which is very close to 4 (flawless). It also took less time to run, with a time of 503 seconds, which is significantly less than ChatDev. \n",
      "\n",
      "Although MetaGPT required more tokens (24,613 or 31,255 compared to 19,292), it only needed 126.5/124.3 tokens to generate one line of code, while ChatDev used 248.9 tokens. The cost of human revision for MetaGPT was also lower. \n",
      "\n",
      "Furthermore, MetaGPT demonstrated superior performance when used with GPT-4 as the backend on selected SoftwareDev tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me the results over one of the above datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower-Level: Debuggability and Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], llm=llm, verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT, \"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "In MetaGPT, various roles are assigned to different agents, such as Product Manager, Architect, Engineer, and others. These roles help in decomposing complex tasks into specific actionable procedures. Each agent has a specific role and expertise, following established standards. This allows for automatic requirement analysis, system design, code generation, modification, execution, and debugging during runtime. The addition of roles different from just the Engineer consistently improves both revisions and executability. While more roles slightly increase the expenses, the overall performance improves noticeably.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task 7b34a62a-dcc5-4d7e-b0b2-6591ff920244: 1\n",
      "In MetaGPT, various roles are assigned to different agents, such as Product Manager, Architect, Engineer, and others. These roles help in decomposing complex tasks into specific actionable procedures. Each agent has a specific role and expertise, following established standards. This allows for automatic requirement analysis, system design, code generation, modification, execution, and debugging during runtime. The addition of roles different from just the Engineer consistently improves both revisions and executability. While more roles slightly increase the expenses, the overall performance improves noticeably.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num upcoming steps for task 7b34a62a-dcc5-4d7e-b0b2-6591ff920244: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='7b34a62a-dcc5-4d7e-b0b2-6591ff920244', step_id='7bac4211-f033-4807-99b3-0b479aaf1b35', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What about how agents share information?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"communication between agents in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "In the MetaGPT framework, communication between agents is facilitated through defined development and communication protocols. This allows for collaboration among various agents, enabling them to work together to accomplish more complex tasks or projects.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"What about how agents share information?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "In MetaGPT, the agents communicate with each other through established development and communication protocols. These protocols enable the agents to collaborate effectively, allowing them to work together to accomplish more complex tasks or projects. This communication is crucial for the functioning of the system as it allows for the decomposition of complex tasks into specific actionable procedures, which are then handled by the agents according to their specific roles and expertise.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In MetaGPT, the agents communicate with each other through established development and communication protocols. These protocols enable the agents to collaborate effectively, allowing them to work together to accomplish more complex tasks or projects. This communication is crucial for the functioning of the system as it allows for the decomposition of complex tasks into specific actionable procedures, which are then handled by the agents according to their specific roles and expertise.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
