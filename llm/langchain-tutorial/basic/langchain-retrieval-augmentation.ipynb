{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10]\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/week/Workspaces/github-projects/notimetowaste/llm/langchain-tutorial/.venv/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d378531fc3254fb193a4934664baeee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10]\")\n",
    "embedding = TextEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object TextEmbedding.embed at 0x7e557409b5b0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[0][\"text\"]\n",
    "embedding.embed(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken  # !pip install tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "tiktoken_len(\n",
    "    \"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "    \"we can find the length of this chunk of text in tokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(data[6][\"text\"])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "embed = FastEmbedEmbeddings()\n",
    "texts = [\"this is the first chunk of text\", \"then another second chunk of text is here\"]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "texts = [\"this is the first chunk of text\", \"then another second chunk of text is here\"]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"langchain-retrieval-augmentation\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 1000\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        \"wiki-id\": str(record[\"id\"]),\n",
    "        \"source\": record[\"url\"],\n",
    "        \"title\": record[\"title\"],\n",
    "    }\n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record[\"text\"])\n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [\n",
    "        {\"chunk\": j, \"text\": text, **metadata} for j, text in enumerate(record_texts)\n",
    "    ]\n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # if we have reached the batch_limit we can add texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            embeddings=embeds,\n",
    "            metadatas=metadatas,\n",
    "            documents=texts,\n",
    "        )\n",
    "        texts = []\n",
    "        metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def embed_text(text):\n",
    "    embeddings = await embed(text)\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "async def main():\n",
    "    text = \"This is a sample text.\"\n",
    "    embeddings = await embed_text(text)\n",
    "    print(embeddings)\n",
    "\n",
    "# Run the async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "# collection = client.create_collection(name=\"ollama\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    d = record[\"text\"]\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", prompt=d)\n",
    "    embedding = response[\"embedding\"]\n",
    "    collection.add(ids=[str(i)], embeddings=[embedding], documents=[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.HttpClient()\n",
    "collection = client.create_collection(name=\"ollama\")\n",
    "\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "batch_size = 100  # adjust this based on your system's memory\n",
    "for i in range(0, len(data), batch_size):\n",
    "    batch = data[i : i + batch_size]\n",
    "    ids = [str(j) for j in range(i, i + len(batch))]\n",
    "    documents = [data[i][\"text\"] for i in range(i, i + len(batch))]\n",
    "    embeddings = [\n",
    "        ollama.embeddings(model=\"nomic-embed-text\", prompt=d)[\"embedding\"]\n",
    "        for d in documents\n",
    "    ]\n",
    "    collection.add(ids=ids, embeddings=embeddings, documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all collections\n",
    "client.list_collections()\n",
    "\n",
    "# get the collection\n",
    "collection = client.get_collection(\"ollama\")\n",
    "\n",
    "# get the document by id\n",
    "doc = collection.get(\"1\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "docs = load_dataset(f\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train[:100]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
