{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# Create the prompte from the template.\n",
    "promptTemplate = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "    not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "\n",
    "     \"\"\"\n",
    "modelSel = \"\"\n",
    "\n",
    "\n",
    "# Load the PDF file to ChromaDB\n",
    "def loadDataFromPDFFile(filePath):\n",
    "    loader = PyPDFLoader(filePath)\n",
    "    pages = loader.load_and_split()\n",
    "    chunks = filter_complex_metadata(pages)\n",
    "    embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    vector_store = Chroma.from_documents(documents=chunks, embedding=embedding)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def modelResponse(message, history):\n",
    "    llm = ChatOllama(model=\"llama3:8b-instruct-q6_K\")\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=promptTemplate, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Initiate the retriever\n",
    "    dbLoaded = loadDataFromPDFFile(\"../data/HP_Book1_Chapter_Excerpt.pdf\")\n",
    "    retriever = dbLoaded.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"k\": 5, \"score_threshold\": 0.2},\n",
    "    )\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(message)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # read configuration file\n",
    "    # conf = {}\n",
    "    # with open(\"config.json\", \"r\") as confFile:\n",
    "    #     conf = json.load(confFile)\n",
    "    #     print(conf[\"model\"])\n",
    "\n",
    "    chatUI = gr.ChatInterface(fn=modelResponse, title=\"Harry Potter Story Q&A\")\n",
    "    chatUI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebBaseLoader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://wix-doc.com/cv/\"],\n",
    "    # bs_kwargs=dict(\n",
    "    #     parse_only=bs4.SoupStrainer(\n",
    "    #         class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "    #     )\n",
    "    # ),\n",
    ")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Wickes Wong\\nlinkedin.com/in/wickes-w  | wickeswong@gmail.com  | wix-doc.com | github.com/wickes1\\nSkills\\nDevelopment: Python, Go, JavaScript, TypeScript, RESTful API, Git, Shell Scripting, FastAPI, Next.js, React\\nData and Analytics:  Pandas, PySpark, Airflow, Kafka, SQL, Grafana, Tableau, PostgreSQL, LangChain\\nInfrastructure: AWS (S3, Lambda, EKS, ECS, Route53, RDS), Docker, Kubernetes, Ansible, Terraform, Helm, Kustomize, \\nGitHub Actions, GitLab CI/CD, ArgoCD\\nExperience\\nData Engineer, Varadise Limited June 2021 â€“ May 2023\\n\\uf06cLed the implementation of an event-driven message distribution system with Kafka and AWS EKS, enabling real-\\ntime and cross-channel event notifications. Achieved sub-second message consumption and production (under \\n100 milliseconds) for high-performance event delivery. Reduced customization time for output channels and \\ntemplates from weeks to minutes through Go Templates.\\n\\uf06cImplemented scalable ETL pipelines with Python, Databricks, Airflow, and Kubernetes integrated with Kafka and \\nAWS S3, achieving a 20% increase in efficiency processing 30 GB of data daily. Ensured data quality controls using \\nPydantic, Great Expectations, and DataHub.\\n\\uf06cBuilt serverless, event-driven ETL processes on AWS Lambda, triggered by events from Kafka and AWS Workmail. \\nFacilitated real-time monitoring of workflows through AWS CloudWatch and Grafana dashboards.\\n\\uf06cDeveloped scalable APIs with FastAPI and gRPC for the Unreal Engine 5 and external clients, improved API \\nthroughput by asynchronous programming. Deployed APIs on Kubernetes using Helm Chart, offering \\nfunctionalities such as CRUD operations, gRPC bridge, OAuth2, and Webhook support.\\n\\uf06cCreated interactive dashboards using Grafana for monitoring Kubernetes clusters and deployment resources. \\nLeveraged Kubecost for cost analysis and optimization, resulting in a 30% reduction in infrastructure costs.\\n\\uf06cImplemented CI/CD pipelines using GitLab CI and ArgoCD, along with precommit, reducing deployment time from \\nweek to hours and enhancing Developer Experience (DX).\\n\\uf06cMentored junior data engineers more than 6 months, fostering a collaborative team environment.\\n\\uf06cApplied Agile and Scrum methodologies to streamline the application lifecycle through Jira.\\nProjects\\nHong Kong Canadians Hackathon@Toronto Mar 2024\\n\\uf06cLed the winning team in designing a solution to address a real-life SME challenge with QuickBooks Desktop, \\nhelping the business owner potentially save $1k annually.\\n\\uf06cDeveloped a Java Sync App with QuickBooks Desktop SDK to extract data and inject into Postgres.\\n\\uf06cDeveloped a responsive web application using Next.js featuring Progressive Web App (PWA).\\nGIS Dataset Ingestion\\n\\uf06cDeveloped a data pipeline to ingest, transform, and publish 20GB+ geospatial data from internal and public Spatial \\nData Infrastructures (SDIs) into PostGIS and GeoServer. Enabled OGC Web Services for frontend applications.\\nSharepoint Site Pages Migration\\n\\uf06cCreated a data pipeline for SharePoint pages to Word with DrissionPage and BeautifulSoup for web scraping.\\nExam ChatBot\\n\\uf06cIntegrated GPT-4 and LangChain models with Azure AI Search. Delivered chatbot API as part of MLOps.\\n\\uf06cConducted data cleansing and preprocessing on exam materials for improved retrieval.\\n\\uf06cOptimized prompts for accurate addressing of syllabus-related inquiries.\\nReal-time Finger Vein Verification System  github.com/wickes1/FingerVeinApp\\n\\uf06cDesigned a deep learning-based recognition system using Python and PyTorch, compatible with the Futronic VS80 \\nFinger Vein Scanner, and implemented an innovative enrollment strategy to enhance accuracy and efficiency.\\nEducation\\nCity University of Hong Kong â€“ BEng Computer and Data Engineering Sept 2017 â€“ June 2021', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyPDFLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/cv-resume-wickeswong-2024-04-21.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Wickes Wong\\nlinkedin.com/in/wickes-w  | wickeswong@gmail.com  | wix-doc.com | github.com/wickes1', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Skills', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Development: Python, Go, JavaScript, TypeScript, RESTful API, Git, Shell Scripting, FastAPI,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Next.js, React', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Data and Analytics:  Pandas, PySpark, Airflow, Kafka, SQL, Grafana, Tableau, PostgreSQL, LangChain', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Infrastructure: AWS (S3, Lambda, EKS, ECS, Route53, RDS), Docker, Kubernetes, Ansible, Terraform,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Helm, Kustomize,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='GitHub Actions, GitLab CI/CD, ArgoCD\\nExperience', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Data Engineer, Varadise Limited June 2021 â€“ May 2023', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cLed the implementation of an event-driven message distribution system with Kafka and AWS EKS,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='enabling real-', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='time and cross-channel event notifications. Achieved sub-second message consumption and production', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='(under', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='100 milliseconds) for high-performance event delivery. Reduced customization time for output', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='channels and', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='templates from weeks to minutes through Go Templates.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cImplemented scalable ETL pipelines with Python, Databricks, Airflow, and Kubernetes integrated', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='with Kafka and', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='AWS S3, achieving a 20% increase in efficiency processing 30 GB of data daily. Ensured data quality', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='controls using', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Pydantic, Great Expectations, and DataHub.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cBuilt serverless, event-driven ETL processes on AWS Lambda, triggered by events from Kafka and AWS', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Workmail.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Facilitated real-time monitoring of workflows through AWS CloudWatch and Grafana dashboards.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cDeveloped scalable APIs with FastAPI and gRPC for the Unreal Engine 5 and external clients,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='improved API', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='throughput by asynchronous programming. Deployed APIs on Kubernetes using Helm Chart, offering', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='functionalities such as CRUD operations, gRPC bridge, OAuth2, and Webhook support.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cCreated interactive dashboards using Grafana for monitoring Kubernetes clusters and deployment', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='resources.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Leveraged Kubecost for cost analysis and optimization, resulting in a 30% reduction in', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='infrastructure costs.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cImplemented CI/CD pipelines using GitLab CI and ArgoCD, along with precommit, reducing deployment', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='time from', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='week to hours and enhancing Developer Experience (DX).', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cMentored junior data engineers more than 6 months, fostering a collaborative team environment.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cApplied Agile and Scrum methodologies to streamline the application lifecycle through Jira.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Projects\\nHong Kong Canadians Hackathon@Toronto Mar 2024', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cLed the winning team in designing a solution to address a real-life SME challenge with QuickBooks', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Desktop,', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='helping the business owner potentially save $1k annually.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cDeveloped a Java Sync App with QuickBooks Desktop SDK to extract data and inject into Postgres.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cDeveloped a responsive web application using Next.js featuring Progressive Web App (PWA).', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='GIS Dataset Ingestion', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cDeveloped a data pipeline to ingest, transform, and publish 20GB+ geospatial data from internal', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='and public Spatial', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Data Infrastructures (SDIs) into PostGIS and GeoServer. Enabled OGC Web Services for frontend', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='applications.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Sharepoint Site Pages Migration', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cCreated a data pipeline for SharePoint pages to Word with DrissionPage and BeautifulSoup for web', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='scraping.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Exam ChatBot', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cIntegrated GPT-4 and LangChain models with Azure AI Search. Delivered chatbot API as part of', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='MLOps.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cConducted data cleansing and preprocessing on exam materials for improved retrieval.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cOptimized prompts for accurate addressing of syllabus-related inquiries.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Real-time Finger Vein Verification System  github.com/wickes1/FingerVeinApp', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf06cDesigned a deep learning-based recognition system using Python and PyTorch, compatible with the', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Futronic VS80', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Finger Vein Scanner, and implemented an innovative enrollment strategy to enhance accuracy and', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='efficiency.', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0}),\n",
       " Document(page_content='Education\\nCity University of Hong Kong â€“ BEng Computer and Data Engineering Sept 2017 â€“ June 2021', metadata={'source': '../data/cv-resume-wickeswong-2024-04-21.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    # search_type=\"similarity_score_threshold\",\n",
    "    # search_kwargs={\"k\": 5, \"score_threshold\": 0.2},\n",
    "\n",
    "\n",
    ")\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "promptTemplate = \"\"\"Answer the question as precise as possible using the provided context. If the answer is not contained in the context, say \"answer not available in context\"\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "     \"\"\"\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b-instruct-q6_K\")\n",
    "prompt = PromptTemplate(\n",
    "    template=promptTemplate, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Answer the question as precise as possible using the provided context. If the answer is not contained in the context, say \"answer not available in context\"\\n    Context: first=VectorStoreRetriever(tags=[\\'Chroma\\', \\'OllamaEmbeddings\\'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7a752ab10140>) last=RunnableLambda(format_docs)\\n    Question: filler question\\n    Answer:\\n     ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": retriever | format_docs, \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me something about the candidate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(question)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n",
    "\n",
    "demo.launch(share=True)  # Share your demo with just 1 extra parameter ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to make a recommendation on hiring Wickes Wong. The provided context only shows his professional experience and skills as a Data Engineer, but it's limited in scope and doesn't provide a comprehensive view of his qualifications or fit for your specific needs. It would be best to conduct a more thorough evaluation and assessment before making a hiring decision.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"Who is Wickes Wong?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"Shall I hire him if I am a recruiter?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Wickes Wong\\nlinkedin.com/in/wickes-w  | wickeswong@gmail.com  | wix-doc.com | github.com/wickes1' metadata={'page': 0, 'source': '../data/cv-resume-wickeswong-2024-04-21.pdf'}\n",
      "\n",
      "page_content='Data Engineer, Varadise Limited June 2021 â€“ May 2023' metadata={'page': 0, 'source': '../data/cv-resume-wickeswong-2024-04-21.pdf'}\n",
      "\n",
      "page_content='\\uf06cOptimized prompts for accurate addressing of syllabus-related inquiries.' metadata={'page': 0, 'source': '../data/cv-resume-wickeswong-2024-04-21.pdf'}\n",
      "\n",
      "page_content='Data Infrastructures (SDIs) into PostGIS and GeoServer. Enabled OGC Web Services for frontend' metadata={'page': 0, 'source': '../data/cv-resume-wickeswong-2024-04-21.pdf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in ai_msg_2[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
